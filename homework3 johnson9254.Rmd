---
title: "Stat 3202: Homework 3"
date: "Due Monday, February 27 by 11:59 pm"
author: "Nathan Johnson.9254"
output: pdf_document
---

\



\large
\

#### Instructions
- Replace "FirstName LastName (name.n)" above with your information.
- Provide your solutions below in the spaces marked "Solution:".
- Include any R code that you use to answer the questions; if a numeric answer is required, show how you calculated it in R.
- Knit this document to **pdf** and upload both the **pfd** file and your completed **Rmd** file to Carmen
- Make sure your solutions are clean and easy-to-read by
    - formatting all plots to be appropriately sized, with appropriate axis labels.
    - only including R code that is necessary to answer the questions below.
    - only including R output that is necessary to answer the questions below (avoiding lengthy output).
    - providing short written answers explaining your work, and writing in complete sentences.


Due on Carmen Monday, February 27 by 11:59 pm. All uploads must be .pdf.
Submissions will be accepted for 24 hours past this deadline, with a deduction of 1%
per hour. Absolutely no submissions will be accepted after this grace period.
\

**Concepts \& Application**

In this assignment, you will

* find consistent estimators of several probability distributions.
* find sufficient statistics of several probability distributions.
* Finding Method of Moment (MOM) estimators.
* Finding Maximum likelihood estimators (MLE).


This homework is worth 40 points. 



\newpage

**Question 1**

Consider two samples $x_1,x_2,\cdots,x_{n_x} \stackrel{\mathrm{iid}}{\sim}f_x$ with $E(X)=\mu_x$ and finite $V(X)=\sigma^2_x$, and $y_1,y_2,\cdots,y_{n_y} \stackrel{\mathrm{iid}}{\sim}f_y$ with $E(Y)=\mu_y$ and finite $V(Y)=\sigma^2_y$.

Assuming that the two samples are independent, show that $\bar{x}-\bar{y}$ is consistent estimator for $\mu_x-\mu_y$.




\

**Solution to Question 1**
$E(\bar{x} - \bar{y}) = E(x) - E(y) = \mu_x-\mu_y.$ Since $E(\bar{x} - \bar{y})$ equals the parameter, it is unbiased.
$V(\bar{x} - \bar{y}) = V(\bar{x}) +V(\bar{y})+2cov(\bar{x},\bar{y}) = \frac{V(x)}{n}+\frac{V(y)}{n}$ As n approaches infinity, $V(\bar{x}-\bar{y})$ approaches 0 so it is consistent. 




\vspace{1cm}

**Question 2**



(a) Consider a sample $x_1,x_2,\cdots,x_n\stackrel{\mathrm{iid}}{\sim}Unif(\theta, \theta+1)$. Show that $\hat{\theta}_{MOM}=\bar{x}-\frac{1}{2}$. You may use known facts about $E(X)$.
(b)	Prove that $\hat{\theta}_{MOM}$ is consistent estimator for $\theta$. You may use known facts about $E(X),E(\bar{X}),V(X),V(\bar{X})$.

\

**Solution to Question 2**

Your answers go here.

**Part a:**
$E(X) = \bar{x} = \frac{2\theta + 1}{2}$ Solve for $\hat\theta$ and you get $\hat\theta_{MOM} = \bar{x} - \frac{1}{2}.$

**Part b:**
$E(\hat\theta) = E(x) - \frac{1}{2} = \theta$. Since $E(\hat\theta)$ is equal to the parameter, it is unbiased.
$V(\hat\theta) = \frac{V(X)}{n} = \frac{\frac{1}{12}}{n}$. As n approaches infinity, $V(\hat\theta)$ will approach 0 so it is consistent.



\vspace{1cm}

**Question 3**


(a) Let $x_1,x_2,\cdots,x_n\stackrel{\mathrm{iid}}{\sim}Gamma(\alpha, \beta)$ where **$\beta$ is known**. Show that $\sum_{1=1}^{n}ln(x_i)$ is sufficient for $\alpha$.
(b) Let $x_1,x_2,\cdots,x_n\stackrel{\mathrm{iid}}{\sim}\chi^2_k$. Show that $\sum_{1=1}^{n}ln(x_i)$ is sufficient for $k$.
(c) Let $x_1,x_2,\cdots,x_n\stackrel{\mathrm{iid}}{\sim}N(\mu, \sigma^2)$ where **$\mu$ is known**. Show that $\sum_{1=1}^{n}(x_i-\mu)^2$ is sufficient for $\sigma^2$.



\

**Solution to Question 3**

Your answers go here.

**Part a:**
$L(\alpha,\beta;\sum_{i=1}^nln(x_i))=\prod_{i=1}^n\frac{(\alpha-1)\beta^\alpha}{(\alpha-1)!}\sum_{i=1}^nln(x_i)e^{-\beta\sum_{i=1}^nln(x_i)}.$ Everything before the sum is $g(u,\alpha)$ and everything after and including the sum is $h(\sum_{i=1}^nln(x_i))$. So it is sufficient.
**Part b:**
$L(k;\sum_{i=1}^nln(x_i)) = \prod_{i=1}^n(\frac{\frac{k}{2}-1}{2^{\frac{k}{2}}(\frac{k}{2}-1)!})\sum_{i=1}^nln(x_i)e^\frac{-\sum_{i=1}^nln(x_i)}{2}$ Everything before the sum is $g(u,k)$ and everything after and including the sum is $h(\sum_{i=1}^nln(x_i))$. So it is sufficient.
**Part c:**
$L(\sigma^2;\sum_{i=1}^n(x_i-\mu)^2)=\prod_{i=1}^n\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-1}{2}\frac{(\sum_{i=1}^n(x_i-\mu)^2-\mu)}{\sigma}^2}$. $\sigma$ can be rewritten as $\sqrt{\frac{\sum_{i=1}^n(x_i-\mu)^2}{n-1}}$ which n and mu are known so $g(u,\sigma)=1$ and $h(\sum_{i=1}^n(x_i-\mu)^2)=\prod_{i=1}^n\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-1}{2}\frac{(\sum_{i=1}^n(x_i-\mu)^2-\mu)}{\sigma}^2}$.



\vspace{1cm}

\newpage

**Question 4**


a) Take an iid sample of size $n$ from exponential distribution, $Exp(\lambda)$. Show the sum of the observations is sufficient for $\lambda$.
b) Compute a method of moments estimator for $\lambda$. You can use known facts about $Exp(\lambda)$.
c) Compute a maximum likelihood estimator for $\lambda$.

\

**Solution to Question 4**

Your answers go here.

**Part a:**
$g(u,\lambda) = 1. h(\sum_{i=1}^nx_i) = \prod_{i=1}^n{\lambda}e^{-\lambda\sum_{i=1}^nx_i}$. Since it can be split up, it is sufficient.
**Part b:**
$\bar{x}=\frac{1}{\lambda}$ so $\lambda_{MOM} = \frac{1}{\bar{x}}$.
**Part c:**
$L(\lambda;x_1,...,x_n)=\lambda^ne^{-n\lambda\sum_{i=1}^nx_i}$
$\frac{d(ln(L(\lambda;x_1,...,x_n))}{d\lambda}=\frac{n}{\lambda}-n\sum_{i=1}^nx_i$
Set the derivative equal to 0 and solve for $\lambda$ and you get $\lambda=\frac{1}{\sum_{i=1}^nx_i}$



\vspace{1cm}

**Question 5**


a) Let $x_1,x_2,\cdots,x_n\stackrel{\mathrm{iid}}{\sim}\text{Laplace}(\mu=0, \sigma)$ where $f(x\mid \mu=0, \sigma)=\frac{1}{2\sigma}e^{-\frac{|x|}{\sigma}}$. Compute $\hat{\sigma}_{MLE}$.
b) Let $x_1,x_2,\cdots,x_n\stackrel{\mathrm{iid}}{\sim}\text{Bernoulli}(p)$. Compute $\hat{p}_{MLE}$.
c) Let $x_1,x_2,\cdots,x_n\stackrel{\mathrm{iid}}{\sim}\text{N}(\mu,\sigma^2)$. Compute $\hat{\mu}_{MLE}$.

    Is $\hat{\mu}_{MLE}$ biased or unbiased?
d) Let $x_1,x_2,\cdots,x_n\stackrel{\mathrm{iid}}{\sim}\text{N}(\mu,\sigma^2)$. Compute $\hat{\sigma}^2_{MLE}$. Hint: let $v=\sigma^2$. This makes the calculus and algebra a bit easier. 

    Is $\hat{\sigma}^2_{MLE}$ biased or unbiased?
\

**Solution to Question 5**

Your answers go here.

**Part a:**
$L(\sigma;x_1,...,x_n) = \frac{1}{(2\sigma)^n}e^{-\frac{\sum_{i=1}^n|x_i|}{\sigma}}$
Then take the derivative of $ln(L(\sigma;x_1,...,x_n))$ and solve for $\sigma$ at 0 and you get:
$\hat\sigma_{MLE}=\frac{\sum_{i=1}^n|x_i|}{n}$.
**Part b:**
$L(p;x_1,...,x_n)=p^{\sum_{i=1}^nx_i}(1-p)^{n-\sum_{i=1}^nx_i}$
Then take the derivative of $ln(L(p;x_1,...,x_n))$ and solve for p at 0 and you get:
$\hat{p}_{MLE}=\frac{\sum_{i=1}^nx_i}{n}$
**Part c:**
$L(\mu,\sigma^2;x_1,...x_n)=(2\pi\sigma^2)^{\frac{-n}{2}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}$.
Take derivative of $ln(L(\mu,\sigma^2;x_1,...x_n))$ and solve for $\mu$ at 0 and you get:
$\hat\mu_{MLE}=\frac{\sum_{i=1}^nx_i}{2n\sigma^2}$.
$E(\frac{\sum_{i=1}^nx_i}{2n\sigma^2})=\frac{1}{2n\sigma^2}\sum_{i=1}^nE(x_i)=\frac{\mu}{2\sigma^2}$. Since $\hat\mu_{MLE}$ does not equal $\mu$, it is biased.
**Part d:**
As taken from above:
$L(\mu,\sigma^2;x_1,...x_n)=(2\pi\sigma^2)^{\frac{-n}{2}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}$.
Take derivative of $ln(L(\mu,\sigma^2;x_1,...x_n))$ and solve for $v=\sigma^2$ at 0 and you get:
$\hat\sigma^2_{MLE}=\frac{\sum_{i=1}^n(x_i-\mu)^2}{n}$.
$E(\frac{\sum_{i=1}^n(x_i-\mu)^2}{n})=\frac{1}{n}\sum_{i=1}^nE((x_i-\mu)^2)=\sigma^2+\mu^2$ which is not $\sigma^2$ so it is biased.








